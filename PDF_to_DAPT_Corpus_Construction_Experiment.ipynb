{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNazA2a7aKoRnGwm0Ru9alo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yash4rma/PDF-to-DAPT-Corpus-Construction-Experiment/blob/main/PDF_to_DAPT_Corpus_Construction_Experiment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y datatrove\n",
        "!pip install datatrove[io]==0.2.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Z7HGXxl9WraT",
        "outputId": "50d990ba-f04b-4816-c0c8-210495d8e3b2",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping datatrove as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting datatrove==0.2.0 (from datatrove[io]==0.2.0)\n",
            "  Downloading datatrove-0.2.0-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: dill>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datatrove==0.2.0->datatrove[io]==0.2.0) (0.3.8)\n",
            "Requirement already satisfied: fsspec>=2023.12.2 in /usr/local/lib/python3.12/dist-packages (from datatrove==0.2.0->datatrove[io]==0.2.0) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.12/dist-packages (from datatrove==0.2.0->datatrove[io]==0.2.0) (1.3.4)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.12/dist-packages (from datatrove==0.2.0->datatrove[io]==0.2.0) (4.15.0)\n",
            "Collecting loguru>=0.7.0 (from datatrove==0.2.0->datatrove[io]==0.2.0)\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.12/dist-packages (from datatrove==0.2.0->datatrove[io]==0.2.0) (0.70.16)\n",
            "Requirement already satisfied: numpy>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from datatrove==0.2.0->datatrove[io]==0.2.0) (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from datatrove==0.2.0->datatrove[io]==0.2.0) (4.67.1)\n",
            "Collecting faust-cchardet (from datatrove[io]==0.2.0)\n",
            "  Downloading faust_cchardet-2.1.19-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (from datatrove[io]==0.2.0) (18.1.0)\n",
            "Collecting python-magic (from datatrove[io]==0.2.0)\n",
            "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting warcio (from datatrove[io]==0.2.0)\n",
            "  Downloading warcio-1.7.5-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: datasets>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from datatrove[io]==0.2.0) (4.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets>=2.18.0->datatrove[io]==0.2.0) (3.20.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets>=2.18.0->datatrove[io]==0.2.0) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.18.0->datatrove[io]==0.2.0) (2.32.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets>=2.18.0->datatrove[io]==0.2.0) (3.6.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets>=2.18.0->datatrove[io]==0.2.0) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets>=2.18.0->datatrove[io]==0.2.0) (6.0.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.17.0->datatrove==0.2.0->datatrove[io]==0.2.0) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.17.0->datatrove==0.2.0->datatrove[io]==0.2.0) (0.28.1)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.17.0->datatrove==0.2.0->datatrove[io]==0.2.0) (1.5.4)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.17.0->datatrove==0.2.0->datatrove[io]==0.2.0) (0.21.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.17.0->datatrove==0.2.0->datatrove[io]==0.2.0) (4.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from warcio->datatrove[io]==0.2.0) (1.17.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0->datatrove[io]==0.2.0) (3.13.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.17.0->datatrove==0.2.0->datatrove[io]==0.2.0) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.17.0->datatrove==0.2.0->datatrove[io]==0.2.0) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.17.0->datatrove==0.2.0->datatrove[io]==0.2.0) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.17.0->datatrove==0.2.0->datatrove[io]==0.2.0) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.17.0->datatrove==0.2.0->datatrove[io]==0.2.0) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.18.0->datatrove[io]==0.2.0) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets>=2.18.0->datatrove[io]==0.2.0) (2.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.18.0->datatrove[io]==0.2.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.18.0->datatrove[io]==0.2.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets>=2.18.0->datatrove[io]==0.2.0) (2025.3)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface-hub>=0.17.0->datatrove==0.2.0->datatrove[io]==0.2.0) (8.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0->datatrove[io]==0.2.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0->datatrove[io]==0.2.0) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0->datatrove[io]==0.2.0) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0->datatrove[io]==0.2.0) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0->datatrove[io]==0.2.0) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0->datatrove[io]==0.2.0) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=2.18.0->datatrove[io]==0.2.0) (1.22.0)\n",
            "Downloading datatrove-0.2.0-py3-none-any.whl (16.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faust_cchardet-2.1.19-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (317 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
            "Downloading warcio-1.7.5-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.6/40.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faust-cchardet, warcio, python-magic, loguru, datatrove\n",
            "Successfully installed datatrove-0.2.0 faust-cchardet-2.1.19 loguru-0.7.3 python-magic-0.4.27 warcio-1.7.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 1: DataTrove environment reset and version pinning.\n",
        "\n",
        "\n",
        "*   This step removes any pre-installed DataTrove version and installs a pinned\n",
        "*   public release (datatrove[io]==0.2.0) to test compatibility with FineWeb-style pipelines.\n",
        "*   Output: DataTrove 0.2.0 installed successfully, confirming the runtime environment state.\n",
        "\n",
        "Note: This step later revealed API limitations in the public DataTrove release.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IYrHATrOj-kC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zQHxvxcwUFxu",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a373effa-ecfa-42e7-ab6b-f0754b628e8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.1/329.1 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q \\\n",
        "  pypdf \\\n",
        "  langdetect \\\n",
        "  tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 2: Install core processing libraries\n",
        "This step installs supporting libraries required for the pipeline:\n",
        "- pypdf: extract text content from PDF files\n",
        "- langdetect: detect document language for quality filtering\n",
        "- tqdm: provide progress visibility during processing\n",
        "Output: All auxiliary dependencies installed and available in the runtime."
      ],
      "metadata": {
        "id": "s6KsAl21kj3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"/content/datatrove_experiment\")\n",
        "\n",
        "DIRS = {\n",
        "    \"raw_pdfs\": BASE / \"01_raw_pdfs\",\n",
        "    \"raw_text\": BASE / \"02_raw_text\",\n",
        "    \"jsonl_input\": BASE / \"03_jsonl_input\",\n",
        "    \"normalized\": BASE / \"04_normalized\",\n",
        "    \"filtered\": BASE / \"05_filtered\",\n",
        "    \"deduped\": BASE / \"06_deduped\",\n",
        "    \"final\": BASE / \"07_final_corpus\",\n",
        "    \"logs\": BASE / \"logs\"\n",
        "}\n",
        "\n",
        "for d in DIRS.values():\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(\"Directory structure ready\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_Xlg3cZqVBo2",
        "outputId": "1ca1ea2c-6cfb-4ef6-da17-41549feef43f",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory structure ready\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 3: Directory structure initialization\n",
        "\n",
        "*   This step defines and creates a staged directory layout for the corpus pipeline.\n",
        "*   Each directory represents a processing stage and stores its intermediate artifacts.\n",
        "*   Output: Directory structure created under /content/datatrove_experiment, including folders for raw PDFs, extracted text, JSONL inputs, normalized data, filtered data, deduplicated data, final corpus output, and logs.\n",
        "\n"
      ],
      "metadata": {
        "id": "VF6XgUfJkpUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "from pathlib import Path\n",
        "\n",
        "uploaded_files = [\n",
        "    \"/content/Pharma-test01.pdf\",\n",
        "    \"/content/Pharma-test02.pdf\",\n",
        "    \"/content/Finance-test01.pdf\",\n",
        "    \"/content/Finance-test02.pdf\",\n",
        "]\n",
        "\n",
        "for f in uploaded_files:\n",
        "    shutil.copy(f, DIRS[\"raw_pdfs\"])\n",
        "\n",
        "# Verify copy\n",
        "list(DIRS[\"raw_pdfs\"].iterdir())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "cS5dy5hjVHcV",
        "outputId": "8f5528b0-d499-4391-bf58-f7feca77a004",
        "collapsed": true
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PosixPath('/content/datatrove_experiment/01_raw_pdfs/Pharma-test02.pdf'),\n",
              " PosixPath('/content/datatrove_experiment/01_raw_pdfs/Pharma-test01.pdf'),\n",
              " PosixPath('/content/datatrove_experiment/01_raw_pdfs/Finance-test01.pdf'),\n",
              " PosixPath('/content/datatrove_experiment/01_raw_pdfs/Finance-test02.pdf')]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 4 – Copy uploaded PDF files into the pipeline workspace\n",
        "\n",
        "This step copies the PDF files that were uploaded directly into the Google Colab environment into the pipeline’s `01_raw_pdfs` directory. This ensures that all raw input documents are stored in a controlled, versioned location and that all subsequent processing stages operate only on pipeline-managed files.\n",
        "\n",
        "**Output:**  \n",
        "All input PDF files (`Pharma-test01.pdf`, `Pharma-test02.pdf`, `Finance-test01.pdf`, `Finance-test02.pdf`) are copied into the `01_raw_pdfs` directory, and the copy is verified by listing the directory contents."
      ],
      "metadata": {
        "id": "e1xl0613lhfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pypdf import PdfReader\n",
        "from tqdm import tqdm\n",
        "\n",
        "def pdf_to_text(pdf_path):\n",
        "    reader = PdfReader(pdf_path)\n",
        "    pages = []\n",
        "    for page in reader.pages:\n",
        "        txt = page.extract_text()\n",
        "        if txt:\n",
        "            pages.append(txt)\n",
        "    return \"\\n\".join(pages)\n",
        "\n",
        "for pdf in tqdm(DIRS[\"raw_pdfs\"].glob(\"*.pdf\")):\n",
        "    text = pdf_to_text(pdf)\n",
        "    out = DIRS[\"raw_text\"] / f\"{pdf.stem}.txt\"\n",
        "    out.write_text(text, encoding=\"utf-8\")\n",
        "\n",
        "print(\"Raw text extraction complete\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "SaXtNQMvVnkG",
        "outputId": "80bb068a-68ce-4980-ab01-eb85d9345f11",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "4it [00:39,  9.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw text extraction complete\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 5 – Extract raw text from PDF documents\n",
        "\n",
        "This step performs format-aware text extraction from each PDF file using the `pypdf` library. Each page of the document is processed sequentially, and any extractable text is collected and concatenated. No cleaning, filtering, or normalization is applied at this stage in order to preserve the original document content.\n",
        "\n",
        "**Output:**  \n",
        "One raw text file per PDF is generated and stored in the `02_raw_text` directory (for example, `Pharma-test01.txt` and `Finance-test02.txt`). These files contain the unprocessed text extracted directly from the PDFs."
      ],
      "metadata": {
        "id": "ETvqHoZelr9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print((DIRS[\"raw_text\"] / \"Pharma-test01.txt\").read_text()[:2000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Fb5IArYQV-NT",
        "outputId": "1cc2fee1-6310-462d-8ba9-79ffab3132b0",
        "collapsed": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CENTER FOR DRUG EVALUATION AND \n",
            "RESEARCH \n",
            "A\n",
            "PPLICATION NUMBER: \n",
            "214787Orig1s000 \n",
            "CLINICAL PHARMACOLOGY \n",
            "REVIEW(S) \n",
            " \n",
            "1 \n",
            " \n",
            "Date: July 16, 2020   \n",
            " \n",
            "From: Neil Hartman, PhD, Marlene Kim, PhD, Naomi Kruhlak, PhD, Rebecca Racz, PharmD, Division of \n",
            "Applied Regulatory Science/Office of Clinical Pharmacology (DARS/OCP) \n",
            " \n",
            "Through: James Weaver Ph.D., Consult Lead and David Strauss M.D., Ph.D., Director; DARS/OCP  \n",
            " \n",
            "To: Neha Gada, Division of Pharmacovigilance II, Office of Surveillance and Epidemiology \n",
            " \n",
            "Subject:  In silico Analyses on the Potential Association of Remdesivir with Renal and Hepatic Events \n",
            "(NDA 21487) \n",
            " \n",
            "Executive Summary \n",
            " \n",
            "Remdesivir is currently approved under an Emergency Use Authorization (EUA) for COVID-19. It is \n",
            "closely related to adenosine and the adenosine nucleotides in structure.  Multiple adverse events have been \n",
            "reported to the Agency, including acute kidney injury.  Additionally, the Emergency Use Authorization \n",
            "describes a known risk of increased transaminase elevations during remdesivir treatment.  However, as \n",
            "COVID-19 can also lead to serious outcomes, including acute kidney injury, the contribution of remdesivir \n",
            "to these adverse events is unclear. DARS performed multiple computational analyses, primarily based on \n",
            "structural similarity to other drugs, to evaluate remdesivir’s potential association with these adverse events.  \n",
            " \n",
            "In silico pharmacologic target predictions performed using the software platform Clarity identified DNA \n",
            "polymerase as well as several adenosine and purine receptors as possible secondary targets of remdesivir \n",
            "and its metabolites. The sponsor-provided in vitro data found that most of these predictions were negative. \n",
            "Remdesivir did interact with DNA polymerase in vitro at clinically-relevant concentrations; however, \n",
            "development of toxicity through this mechanism generally requires chronic exposure and remdesivir is \n",
            "rapidly converted to its metabolites, which were not associated with DNA polymerase at clini\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 6 – Validate raw text extraction\n",
        "\n",
        "This step performs a manual sanity check on the extracted text by reading and displaying a portion of one text file. This verification ensures that the PDF-to-text extraction step completed successfully and that the extracted content is readable and representative of the source document.\n",
        "\n",
        "**Output:**  \n",
        "A preview of the extracted text is displayed in the notebook output, confirming that raw text extraction completed correctly and produced usable text."
      ],
      "metadata": {
        "id": "5ZiUc8L9luU7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "jsonl_path = DIRS[\"jsonl_input\"] / \"documents.jsonl\"\n",
        "\n",
        "with open(jsonl_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for txt in DIRS[\"raw_text\"].glob(\"*.txt\"):\n",
        "        domain = \"pharma\" if \"Pharma\" in txt.name else \"finance\"\n",
        "        record = {\n",
        "            \"id\": txt.stem,\n",
        "            \"text\": txt.read_text(),\n",
        "            \"domain\": domain,\n",
        "            \"source\": \"pdf\"\n",
        "        }\n",
        "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "jsonl_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "KTS3ngNYV_tI",
        "outputId": "27db448f-f553-45dd-b0ab-17881d6b3e6a",
        "collapsed": true
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content/datatrove_experiment/03_jsonl_input/documents.jsonl')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 7 – Convert extracted text files into structured JSONL format\n",
        "\n",
        "This step transforms the raw text files produced from PDF extraction into a structured JSONL dataset. Each document is wrapped as a single JSON object containing an identifier, the full text content, a domain label (pharma or finance inferred from the filename), and the data source. This step establishes a standardized schema that all downstream processing stages rely on.\n",
        "\n",
        "**Output:**  \n",
        "A single JSONL file (`documents.jsonl`) is created inside the `03_jsonl_input` directory, where each line represents one document with metadata and text content."
      ],
      "metadata": {
        "id": "0hYJE0PTmCQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show datatrove\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "JYNsIcLSY17b",
        "outputId": "4b959f25-2795-43bf-f1d4-742f4e085f56",
        "collapsed": true
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: datatrove\n",
            "Version: 0.2.0\n",
            "Summary: HuggingFace library to process and filter large amounts of webdata\n",
            "Home-page: \n",
            "Author: \n",
            "Author-email: \"HuggingFace Inc.\" <guilherme@huggingface.co>\n",
            "License: Apache-2.0\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: dill, fsspec, huggingface-hub, humanize, loguru, multiprocess, numpy, tqdm\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 8 – Verify installed DataTrove version\n",
        "\n",
        "This step checks the installed DataTrove package version using `pip show`. The purpose is to confirm the exact public version available in the Colab runtime and to document the environment state for reproducibility and debugging.\n",
        "\n",
        "**Output:**  \n",
        "The DataTrove package metadata is displayed, including the version number (`0.2.0`), installation location, and dependency information."
      ],
      "metadata": {
        "id": "-MZONFmemIuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "import unicodedata"
      ],
      "metadata": {
        "id": "c9MqfS_LcfHm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 9 – Import libraries for text normalization\n",
        "\n",
        "This step imports standard Python libraries required for text normalization, including JSON handling, regular expressions, and Unicode utilities. These libraries are used to implement explicit and transparent text-cleaning logic without relying on external processing frameworks.\n",
        "\n",
        "**Output:**  \n",
        "Required standard libraries are successfully loaded into the runtime, preparing the environment for normalization."
      ],
      "metadata": {
        "id": "C97FHs0cmNcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(text):\n",
        "    text = unicodedata.normalize(\"NFC\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text.strip()\n",
        "\n",
        "input_file = DIRS[\"jsonl_input\"] / \"documents.jsonl\"\n",
        "norm_file = DIRS[\"normalized\"] / \"normalized.jsonl\"\n",
        "\n",
        "with open(input_file) as fin, open(norm_file, \"w\") as fout:\n",
        "    for line in fin:\n",
        "        doc = json.loads(line)\n",
        "        doc[\"text\"] = normalize_text(doc[\"text\"])\n",
        "        fout.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "norm_file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "h3DcQjmcc4By",
        "outputId": "0df4aa61-2b10-4061-8b56-7130002fd6b5",
        "collapsed": true
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content/datatrove_experiment/04_normalized/normalized.jsonl')"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 10 – Normalize document text and generate normalized JSONL\n",
        "\n",
        "This step applies text normalization to each document in the JSONL dataset. Unicode normalization (NFC) is applied, excess whitespace is collapsed, and leading/trailing spaces are removed. The normalized text replaces the original text field while preserving all metadata.\n",
        "\n",
        "**Output:**  \n",
        "A new JSONL file (`normalized.jsonl`) is created in the `04_normalized` directory, containing normalized versions of all documents."
      ],
      "metadata": {
        "id": "sWtSGby0mcIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(norm_file) as f:\n",
        "    sample = json.loads(next(f))\n",
        "    print(sample[\"id\"])\n",
        "    print(sample[\"text\"][:600])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "4UWLNIfgc70o",
        "outputId": "244b79dd-4f56-439d-dd2b-e1925c8be705",
        "collapsed": true
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pharma-test01\n",
            "CENTER FOR DRUG EVALUATION AND RESEARCH A PPLICATION NUMBER: 214787Orig1s000 CLINICAL PHARMACOLOGY REVIEW(S) 1 Date: July 16, 2020 From: Neil Hartman, PhD, Marlene Kim, PhD, Naomi Kruhlak, PhD, Rebecca Racz, PharmD, Division of Applied Regulatory Science/Office of Clinical Pharmacology (DARS/OCP) Through: James Weaver Ph.D., Consult Lead and David Strauss M.D., Ph.D., Director; DARS/OCP To: Neha Gada, Division of Pharmacovigilance II, Office of Surveillance and Epidemiology Subject: In silico Analyses on the Potential Association of Remdesivir with Renal and Hepatic Events (NDA 21487) Executiv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 11 – Validate normalized document output\n",
        "\n",
        "This step performs a sanity check on the normalized dataset by reading a single document from the normalized JSONL file and printing its identifier and a text preview. This ensures normalization was applied correctly and that document structure is preserved.\n",
        "\n",
        "**Output:**  \n",
        "A sample document ID and a snippet of normalized text are displayed in the notebook output."
      ],
      "metadata": {
        "id": "gP2omlRGmgY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langdetect import detect"
      ],
      "metadata": {
        "id": "RujNlMesc9kJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 12 – Import language detection utility\n",
        "\n",
        "This step imports the language detection function used to identify the language of document text. This prepares the environment for applying language-based quality filtering in the next stage.\n",
        "\n",
        "**Output:**  \n",
        "The language detection library is successfully loaded and ready for use."
      ],
      "metadata": {
        "id": "vvErmGThmj0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_english(text):\n",
        "    try:\n",
        "        return detect(text) == \"en\"\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "filtered_file = DIRS[\"filtered\"] / \"filtered.jsonl\"\n",
        "\n",
        "with open(norm_file) as fin, open(filtered_file, \"w\") as fout:\n",
        "    for line in fin:\n",
        "        doc = json.loads(line)\n",
        "        text = doc[\"text\"]\n",
        "\n",
        "        if len(text) < 300:\n",
        "            continue\n",
        "\n",
        "        if not is_english(text):\n",
        "            continue\n",
        "\n",
        "        fout.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "filtered_file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "LycVUlgwc_d4",
        "outputId": "7d455be5-66af-42a1-84d3-d117911ce025",
        "collapsed": true
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content/datatrove_experiment/05_filtered/filtered.jsonl')"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 13 – Apply quality filtering based on length and language\n",
        "\n",
        "This step filters the normalized documents using basic quality heuristics. Documents shorter than a minimum length threshold and documents not detected as English are removed. Only documents that pass both checks are retained.\n",
        "\n",
        "**Output:**  \n",
        "A filtered JSONL file (`filtered.jsonl`) is created in the `05_filtered` directory, containing only high-quality English documents."
      ],
      "metadata": {
        "id": "vPykvSbZmnlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q datasketch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "OVhMR8SOdIB_",
        "outputId": "3b1c4a41-267d-40fc-841b-0b3921ff24ee",
        "collapsed": true
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/96.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.5/96.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 14 – Install MinHash-based deduplication dependency\n",
        "\n",
        "This step installs the `datasketch` library, which provides MinHash and Locality Sensitive Hashing (LSH) implementations. These are required for near-duplicate document detection and removal.\n",
        "\n",
        "**Output:**  \n",
        "The `datasketch` library is successfully installed in the Colab environment."
      ],
      "metadata": {
        "id": "tctnHWljmrvJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasketch import MinHash, MinHashLSH"
      ],
      "metadata": {
        "id": "2xObYiOEdAzX"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 15 – Import MinHash and LSH utilities for deduplication\n",
        "\n",
        "This step imports the MinHash and MinHashLSH classes from the `datasketch` library. These utilities are used to compute similarity fingerprints for documents and identify near-duplicates efficiently.\n",
        "\n",
        "**Output:**  \n",
        "MinHash and LSH classes are loaded and available for use in the deduplication stage."
      ],
      "metadata": {
        "id": "136BXeVfmvAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_minhash(text, num_perm=128):\n",
        "    m = MinHash(num_perm=num_perm)\n",
        "    for token in set(text.split()):\n",
        "        m.update(token.encode(\"utf8\"))\n",
        "    return m\n",
        "\n",
        "lsh = MinHashLSH(threshold=0.85, num_perm=128)\n",
        "dedup_file = DIRS[\"deduped\"] / \"deduped.jsonl\"\n",
        "\n",
        "with open(filtered_file) as fin, open(dedup_file, \"w\") as fout:\n",
        "    for i, line in enumerate(fin):\n",
        "        doc = json.loads(line)\n",
        "        mh = get_minhash(doc[\"text\"])\n",
        "\n",
        "        if lsh.query(mh):\n",
        "            continue  # duplicate\n",
        "\n",
        "        lsh.insert(str(i), mh)\n",
        "        fout.write(json.dumps(doc, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "dedup_file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "SgT2gYkjdONw",
        "outputId": "fdf65652-5491-47cb-aab6-12af805ed9cb",
        "collapsed": true
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content/datatrove_experiment/06_deduped/deduped.jsonl')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 16 – Perform near-duplicate document removal using MinHash and LSH\n",
        "\n",
        "This step applies near-duplicate detection to the filtered dataset using MinHash fingerprints and Locality Sensitive Hashing (LSH). Each document’s text is converted into a MinHash signature, which is compared against previously seen documents. Documents that exceed the similarity threshold are treated as duplicates and excluded.\n",
        "\n",
        "**Output:**  \n",
        "A deduplicated JSONL file (`deduped.jsonl`) is created in the `06_deduped` directory, containing only unique or sufficiently distinct documents."
      ],
      "metadata": {
        "id": "2sie-wqamz_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_path = DIRS[\"final\"] / \"final_corpus.jsonl\"\n",
        "\n",
        "with open(dedup_file) as fin, open(final_path, \"w\") as fout:\n",
        "    for line in fin:\n",
        "        fout.write(line)\n",
        "\n",
        "final_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "wHHbcmaTdQgo",
        "outputId": "1f07662b-94a4-4b32-b8f6-ff1b24308cc6",
        "collapsed": true
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('/content/datatrove_experiment/07_final_corpus/final_corpus.jsonl')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CELL 17 – Assemble and freeze the final corpus snapshot\n",
        "\n",
        "This step creates the final, immutable corpus snapshot by copying all deduplicated documents into a single output file. No further processing is applied at this stage, ensuring that the resulting dataset represents the final training-ready corpus.\n",
        "\n",
        "**Output:**  \n",
        "The final corpus file (`final_corpus.jsonl`) is created in the `07_final_corpus` directory. This file contains the cleaned, normalized, filtered, and deduplicated documents and is ready for downstream use such as domain-adaptive pretraining."
      ],
      "metadata": {
        "id": "lSE0-4vom9Yc"
      }
    }
  ]
}